{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lOKPLXaTu51"
      },
      "source": [
        "#Extracting Text Data Using Python\n",
        "\n",
        "Data Storage depends on the type of bussiness, amount of data and cost associated with different sources.\n",
        "* SQL database\n",
        "* Hadoop clusters\n",
        "* cloud storage\n",
        "* flat files\n",
        "\n",
        "Opensoruce Platform where we can extract data for free\n",
        " * Wikipedia\n",
        " * Free APIs like X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noGxasozVMvo"
      },
      "source": [
        "#### Web Scraping\n",
        "Extracting the content/data from websites, blogs, forums, and retail websits for reviews with the permision from the respective sources using web scraping using web scraping packages in python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWh3-JR1VvHD"
      },
      "source": [
        "Step 1 : Data collection\n",
        "\n",
        "Problem : we are read  parse/read HTML Pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EopX8q5aTmwu",
        "outputId": "52de792e-ece6-446b-9be0-c5b296feff12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /home/codespace/.local/lib/python3.12/site-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n"
          ]
        }
      ],
      "source": [
        "# Installation\n",
        "!pip install bs4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti2PqhYaTuMP"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ReHWJAGxWLtN"
      },
      "outputs": [],
      "source": [
        "# Importing the Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import urllib.request as urllib2\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O_nwayNYWLmJ"
      },
      "outputs": [],
      "source": [
        "response = urllib2.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
        "html_doc = response.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG3Pn5g1XDVy",
        "outputId": "9a448b7e-4444-4ccf-c23f-aa1059b333f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<!DOCTYPE html>\n",
            "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\" dir=\"ltr\" lang=\"en\">\n",
            " <head>\n",
            "  <meta charset=\"utf-8\"/>\n",
            "  <title>\n",
            "   Natural language processing - Wikipedia\n",
            "  </title>\n",
            "  <script>\n",
            "   (function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-w\n"
          ]
        }
      ],
      "source": [
        "#Parsing\n",
        "soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "# Formating the parsed html file\n",
        "strhtm = soup.prettify()\n",
        "# Print few lines\n",
        "print (strhtm[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n09kZT4Xji1",
        "outputId": "ac7ab4b4-7571-4f5c-f4aa-7e670866a373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<title>Natural language processing - Wikipedia</title>\n"
          ]
        }
      ],
      "source": [
        "print(soup.title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vmhXARjeYGRz",
        "outputId": "2e1f5bff-e275-4bc1-de94-4ab6361c2741"
      },
      "outputs": [],
      "source": [
        "for t in soup.find_all('a'):\n",
        "  print(t.string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5hEAO1yYWfB",
        "outputId": "60319b06-f5fd-4de6-9d0e-f20fe5bf4e39"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for p in soup.find_all('p'):\n",
        "  data.append(p)\n",
        "  print(p.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'bs4.element.Tag'>\n"
          ]
        }
      ],
      "source": [
        "print(type(data[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipykernel_6183/4186119024.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re.split('\\s+',str(data[0]))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<p><b>Natural',\n",
              " 'language',\n",
              " 'processing</b>',\n",
              " '(<b>NLP</b>)',\n",
              " 'is',\n",
              " 'a',\n",
              " 'subfield',\n",
              " 'of',\n",
              " '<a',\n",
              " 'href=\"/wiki/Computer_science\"',\n",
              " 'title=\"Computer',\n",
              " 'science\">computer',\n",
              " 'science</a>',\n",
              " 'and',\n",
              " 'especially',\n",
              " '<a',\n",
              " 'href=\"/wiki/Artificial_intelligence\"',\n",
              " 'title=\"Artificial',\n",
              " 'intelligence\">artificial',\n",
              " 'intelligence</a>.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'primarily',\n",
              " 'concerned',\n",
              " 'with',\n",
              " 'providing',\n",
              " 'computers',\n",
              " 'with',\n",
              " 'the',\n",
              " 'ability',\n",
              " 'to',\n",
              " 'process',\n",
              " 'data',\n",
              " 'encoded',\n",
              " 'in',\n",
              " '<a',\n",
              " 'href=\"/wiki/Natural_language\"',\n",
              " 'title=\"Natural',\n",
              " 'language\">natural',\n",
              " 'language</a>',\n",
              " 'and',\n",
              " 'is',\n",
              " 'thus',\n",
              " 'closely',\n",
              " 'related',\n",
              " 'to',\n",
              " '<a',\n",
              " 'href=\"/wiki/Information_retrieval\"',\n",
              " 'title=\"Information',\n",
              " 'retrieval\">information',\n",
              " 'retrieval</a>,',\n",
              " '<a',\n",
              " 'class=\"mw-redirect\"',\n",
              " 'href=\"/wiki/Knowledge_representation\"',\n",
              " 'title=\"Knowledge',\n",
              " 'representation\">knowledge',\n",
              " 'representation</a>',\n",
              " 'and',\n",
              " '<a',\n",
              " 'href=\"/wiki/Computational_linguistics\"',\n",
              " 'title=\"Computational',\n",
              " 'linguistics\">computational',\n",
              " 'linguistics</a>,',\n",
              " 'a',\n",
              " 'subfield',\n",
              " 'of',\n",
              " '<a',\n",
              " 'href=\"/wiki/Linguistics\"',\n",
              " 'title=\"Linguistics\">linguistics</a>.',\n",
              " 'Typically',\n",
              " 'data',\n",
              " 'is',\n",
              " 'collected',\n",
              " 'in',\n",
              " '<a',\n",
              " 'href=\"/wiki/Text_corpus\"',\n",
              " 'title=\"Text',\n",
              " 'corpus\">text',\n",
              " 'corpora</a>,',\n",
              " 'using',\n",
              " 'either',\n",
              " 'rule-based,',\n",
              " 'statistical',\n",
              " 'or',\n",
              " 'neural-based',\n",
              " 'approaches',\n",
              " 'in',\n",
              " '<a',\n",
              " 'href=\"/wiki/Machine_learning\"',\n",
              " 'title=\"Machine',\n",
              " 'learning\">machine',\n",
              " 'learning</a>',\n",
              " 'and',\n",
              " '<a',\n",
              " 'href=\"/wiki/Deep_learning\"',\n",
              " 'title=\"Deep',\n",
              " 'learning\">deep',\n",
              " 'learning</a>.',\n",
              " '</p>']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "#run the split query\n",
        "re.split('\\s+',str(data[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fFyzoxlYnkZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
